{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape and create the data frame form scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://perenual.com/plant-species-database-search-finder/species/\"\n",
    "\n",
    "def scrape_page(page_number):\n",
    "\n",
    "    url = f\"{base_url}{page_number}\"\n",
    "\n",
    "    response = requests.get(url) \n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # div containing the data\n",
    "        data_div = soup.find(\"div\", class_=\"text-xs grid md:grid-cols-2 gap-2 bg-gray-100 rounded p-3\")\n",
    "\n",
    "        if data_div:\n",
    "            \n",
    "             # Make dict\n",
    "            page_data = {}\n",
    "            h3_elements = data_div.find_all(\"h3\")\n",
    "            p_elements = data_div.find_all(\"p\")\n",
    "\n",
    "            for h3, p in zip(h3_elements, p_elements):\n",
    "                page_data[h3.text.strip()] = p.text.strip()\n",
    "\n",
    "            return page_data\n",
    "\n",
    "        else:\n",
    "            print(\"Data div not found on the page.\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "start_page =600\n",
    "end_page = 1200\n",
    "\n",
    "# create alist for fontain the dict\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# for loop to scrape data\n",
    "for page_number in range(start_page, end_page + 1):\n",
    "    page_data = scrape_page(page_number)\n",
    "    if page_data:\n",
    "        data_list.append(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_list = pd.DataFrame(data_list)\n",
    "plants_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_list.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_list.to_csv('data_sets/Hardiness_and_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the 2 data frames the we made with the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardinof1 = pd.read_csv('data_sets/Hardiness_and_info.csv')\n",
    "hardinof2 = pd.read_csv('data_sets/Hardiness_and_info2.csv')\n",
    "\n",
    "Hardiness_final = pd.concat([hardinof1,hardinof2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
